# Tier: dev (Mac M4) | test (5070 Ti) | demo (DGX Spark)
TIER=dev

# Inference (Ollama on Mac, vLLM on test/demo)
INFERENCE_URL=http://localhost:11434
MODEL_NAME=llama3.1:8b

# Context
CONTEXT_LENGTH=8192

# Paths (used from Phase 3 onward)
INDEX_PATH=./data/faiss_index
REPO_PATH=./data/transformers
# Ingest: max files to index (0 = full repo; default 500 keeps runs ~1â€“2 min)
INGEST_MAX_FILES=0
