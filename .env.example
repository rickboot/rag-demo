# Tier: dev (Mac M4) | test (5070 Ti) | demo (DGX Spark)
TIER=dev

# Inference (Ollama on Mac, vLLM on test/demo)
INFERENCE_URL=http://localhost:11434
MODEL_NAME=llama3.1:8b

# Context (Ollama: set NUM_CTX for smaller/faster context; leave 0 for model default)
CONTEXT_LENGTH=8192
NUM_CTX=0
NUM_PREDICT=0

# RAG: smaller = faster (RAG_TOP_K chunks, max MAX_CONTEXT_CHARS in prompt)
RAG_TOP_K=4
MAX_CONTEXT_CHARS=8000

# Paths (used from Phase 3 onward)
INDEX_PATH=./data/faiss_index
REPO_PATH=./data/transformers
# Ingest: max files to index (0 = full repo; default 500 keeps runs ~1â€“2 min)
INGEST_MAX_FILES=0
